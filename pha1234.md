Phase 1: Improve streaming & stability (small, high impact)
Real SSE passthrough for Responses API (token-by-token instead of fake two-chunk stream).
Add backoff/retry and clearer error messages in openai_responses_proxy.py.
Basic logging/metrics hooks (structured logs only; no behavior change).
Phase 2: Native file handling (no markers)
Accept real uploads from OpenWebUI and forward to Responses API input_file/input_image directly.
Preserve current PDF marker path as fallback; add size guards and user-facing errors.
Keep existing PPT/PDF filters but ensure they don’t override non-PPT/PDF.
Phase 3: Broader document support (text-first)
Add DOCX/XLSX/CSV ingestion: extract text/tables; attach clean text blocks (no mock data).
Add safe limits (size/page/row caps) and graceful failure logs.
Phase 4: Data analysis tool (sandboxed, minimal)
Lightweight Python “analyze file” tool with CPU/time/memory caps; no persistent state.
Supports CSV/XLSX/TSV/JSON; returns summaries/answers, not files.
Hard error guards; never blocks main thread.
Phase 5: Web browsing/search (guarded)
Add a simple search/browse tool with prompt guards and fallbacks; optional toggle.
Phase 6: Audio (optional)
Mic → transcription, and TTS playback toggle; fail gracefully if services unavailable.
Phase 7: UX improvements
Upload progress/error surfacing.
Chat history search and export/share.
Pinned per-thread system prompt.
Phase 8: Safety & observability
Basic content-filter hooks, rate-limit backoff, and structured metrics (latency, errors, usage counts).
If this order works, I’ll start with Phase 1 (streaming + error handling in the proxy), keeping changes minimal and reversible. Want me to proceed?